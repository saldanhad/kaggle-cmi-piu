{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca39afa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T01:52:49.506528Z",
     "iopub.status.busy": "2024-10-27T01:52:49.506056Z",
     "iopub.status.idle": "2024-10-27T01:52:49.511617Z",
     "shell.execute_reply": "2024-10-27T01:52:49.510743Z"
    },
    "papermill": {
     "duration": 0.012241,
     "end_time": "2024-10-27T01:52:49.513586",
     "exception": false,
     "start_time": "2024-10-27T01:52:49.501345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Feature Engineer Activity Summary of Actigraphy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca3ead4",
   "metadata": {
    "papermill": {
     "duration": 0.002336,
     "end_time": "2024-10-27T01:52:49.518711",
     "exception": false,
     "start_time": "2024-10-27T01:52:49.516375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineer Activity Summary of Actigraphy Data\n",
    "\n",
    "In this notebook, we process actigraphy data from train and test sets using a custom class, `ActivityProcessor`. The main steps are:\n",
    "\n",
    "- **Initialize Processor**:\n",
    "    - Set up paths for train and test data, output directory, and batch size.\n",
    "    - Create a cumulative DataFrame for storing results.\n",
    "\n",
    "\n",
    "- **Batch Processing**:\n",
    "    - **Load Data**: Each batch of parquet files is loaded and concatenated, adding a `student_id` and converting categorical columns as needed.\n",
    "    - **Validate Wear Flag**: Apply thresholds on ENMO and accelerometer data to confirm wear status, generating a `valid_wear_flag`.\n",
    "    - **Summarize Activity**: For each student, calculate wear duration, no-motion periods, and overall wear status.\n",
    "\n",
    "\n",
    "- **Save Results**:\n",
    "    - After processing, save separate summary CSVs for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076d94e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T01:52:49.525349Z",
     "iopub.status.busy": "2024-10-27T01:52:49.524842Z",
     "iopub.status.idle": "2024-10-27T01:52:55.044078Z",
     "shell.execute_reply": "2024-10-27T01:52:55.043274Z"
    },
    "papermill": {
     "duration": 5.525187,
     "end_time": "2024-10-27T01:52:55.046429",
     "exception": false,
     "start_time": "2024-10-27T01:52:49.521242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import glob\n",
    "import os\n",
    "\n",
    "class ActivityProcessor:\n",
    "    \"\"\"\n",
    "    Class to process activity data from train and test parquet files, validate wear flags,\n",
    "    and calculate activity summaries in batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_path, test_path, output_dir, batch_size=100):\n",
    "        \"\"\"\n",
    "        Initializes the ActivityProcessor with paths for train and test data, output directory,\n",
    "        and batch size.\n",
    "        \n",
    "        Parameters:\n",
    "            train_path (str): Path to the directory containing train parquet files.\n",
    "            test_path (str): Path to the directory containing test parquet files.\n",
    "            output_dir (str): Path to the output directory where results will be saved.\n",
    "            batch_size (int): Number of files to process per batch.\n",
    "        \"\"\"\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.activity_summary_df = cudf.DataFrame()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def load_and_concatenate_batch(self, batch_files):\n",
    "        \"\"\"\n",
    "        Loads a batch of parquet files, adds a student_id column to each, and concatenates them.\n",
    "        \n",
    "        Parameters:\n",
    "            batch_files (list): List of parquet file paths to load and concatenate.\n",
    "        \n",
    "        Returns:\n",
    "            cudf.DataFrame: Concatenated DataFrame containing data from all files in the batch.\n",
    "        \"\"\"\n",
    "        batch_df = cudf.DataFrame()\n",
    "        for file in batch_files:\n",
    "            student_id = os.path.basename(os.path.dirname(file))\n",
    "            df = cudf.read_parquet(file)\n",
    "            df['student_id'] = student_id\n",
    "\n",
    "            # Convert categorical columns to strings\n",
    "            for column in df.columns:\n",
    "                if df[column].dtype.name == \"category\":\n",
    "                    df[column] = df[column].astype(\"str\")\n",
    "\n",
    "            batch_df = cudf.concat([batch_df, df], ignore_index=True)\n",
    "        \n",
    "        return batch_df\n",
    "\n",
    "    def validate_non_wear_flag(self, df, enmo_threshold=0.02, std_threshold=0.05, window_size=60):\n",
    "        \"\"\"\n",
    "        Validates wear and non-wear periods by applying thresholds to ENMO and accelerometer std values.\n",
    "        \n",
    "        Parameters:\n",
    "            df (cudf.DataFrame): DataFrame containing activity data.\n",
    "            enmo_threshold (float): Threshold for ENMO values indicating no motion.\n",
    "            std_threshold (float): Threshold for standard deviation indicating no motion.\n",
    "            window_size (int): Rolling window size for calculating standard deviation.\n",
    "        \n",
    "        Returns:\n",
    "            cudf.DataFrame: DataFrame with an additional 'valid_wear_flag' column indicating wear periods.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df['std_X'] = df['X'].rolling(window=window_size).std()\n",
    "        df['std_Y'] = df['Y'].rolling(window=window_size).std()\n",
    "        df['std_Z'] = df['Z'].rolling(window=window_size).std()\n",
    "        df['std_acc'] = (df['std_X'] + df['std_Y'] + df['std_Z']) / 3\n",
    "        df['valid_wear_flag'] = 0\n",
    "        non_wear_mask = (df['enmo'] < enmo_threshold) & (df['std_acc'] < std_threshold)\n",
    "        df['valid_wear_flag'] = df['valid_wear_flag'].mask(non_wear_mask, 1)\n",
    "        df['flag_mismatch'] = df['valid_wear_flag'] != df['non-wear_flag']\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def calculate_activity_summary(self, df):\n",
    "        \"\"\"\n",
    "        Calculates activity summary statistics for each student, including wear duration, no-motion periods,\n",
    "        and overall wear status.\n",
    "        \n",
    "        Parameters:\n",
    "            df (cudf.DataFrame): DataFrame containing validated wear data.\n",
    "        \n",
    "        Returns:\n",
    "            cudf.DataFrame: DataFrame containing summary statistics for each student.\n",
    "        \"\"\"\n",
    "        wear_df = df[df['valid_wear_flag'] == 0]\n",
    "        activity_summary = df.groupby('student_id').agg({\n",
    "            'valid_wear_flag': ['count', 'sum'],\n",
    "            'enmo': 'mean'\n",
    "        })\n",
    "        \n",
    "        activity_summary.columns = ['total_duration', 'non_wear_duration', 'average_activity_level']\n",
    "        activity_summary['wear_duration'] = activity_summary['total_duration'] - activity_summary['non_wear_duration']\n",
    "        no_motion_counts = df[df['enmo'] == 0].groupby('student_id').size().rename('no_motion_count')\n",
    "        activity_summary = activity_summary.merge(no_motion_counts, on='student_id', how='left').fillna(0)\n",
    "        \n",
    "        # Convert durations to seconds\n",
    "        activity_summary['total_duration_seconds'] = activity_summary['total_duration'] * 5\n",
    "        activity_summary['non_wear_duration_seconds'] = activity_summary['non_wear_duration'] * 5\n",
    "        activity_summary['wear_duration_seconds'] = activity_summary['wear_duration'] * 5\n",
    "        activity_summary['no_motion_duration_seconds'] = activity_summary['no_motion_count'] * 5\n",
    "        \n",
    "        # Determine if the device was worn for the majority of the sampling period\n",
    "        activity_summary['overall_wear_status'] = (activity_summary['wear_duration_seconds'] > \n",
    "                                                   (activity_summary['total_duration_seconds'] / 2)).astype(int)\n",
    "        \n",
    "        # Drop sample-based columns\n",
    "        activity_summary = activity_summary.drop(columns=['total_duration', 'non_wear_duration', \n",
    "                                                          'wear_duration', 'no_motion_count']).reset_index()\n",
    "        \n",
    "        return activity_summary\n",
    "\n",
    "    def save_to_csv(self, file_name):\n",
    "        \"\"\"\n",
    "        Saves the cumulative activity summary DataFrame to a CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "            file_name (str): Name of the output CSV file.\n",
    "        \"\"\"\n",
    "        output_path = os.path.join(self.output_dir, file_name)\n",
    "        self.activity_summary_df.to_csv(output_path)\n",
    "        print(f\"Data saved to {output_path}\")\n",
    "\n",
    "    def process_files(self, mode='train'):\n",
    "        \"\"\"\n",
    "        Processes all parquet files in the specified mode (train or test) in batches. For each batch, loads, validates,\n",
    "        and summarizes the data, then appends to the cumulative activity summary DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            mode (str): Indicates which data to process ('train' or 'test').\n",
    "        \"\"\"\n",
    "        data_path = self.train_path if mode == 'train' else self.test_path\n",
    "        parquet_files = glob.glob(os.path.join(data_path, '**/*.parquet'), recursive=True)\n",
    "        \n",
    "        for i in range(0, len(parquet_files), self.batch_size):\n",
    "            batch_files = parquet_files[i:i + self.batch_size]\n",
    "            batch_df = self.load_and_concatenate_batch(batch_files)\n",
    "            validated_df = self.validate_non_wear_flag(batch_df)\n",
    "            batch_summary = self.calculate_activity_summary(validated_df)\n",
    "            self.activity_summary_df = cudf.concat([self.activity_summary_df, batch_summary], ignore_index=True)\n",
    "            print(f\"Processed {mode} batch {i // self.batch_size + 1}/{len(parquet_files) // self.batch_size + 1}\")\n",
    "\n",
    "        # Save to CSV with appropriate naming based on mode\n",
    "        self.save_to_csv(f'activity_summary_{mode}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec13a2fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T01:52:55.054204Z",
     "iopub.status.busy": "2024-10-27T01:52:55.053689Z",
     "iopub.status.idle": "2024-10-27T01:55:27.140489Z",
     "shell.execute_reply": "2024-10-27T01:55:27.139357Z"
    },
    "papermill": {
     "duration": 152.093537,
     "end_time": "2024-10-27T01:55:27.143047",
     "exception": false,
     "start_time": "2024-10-27T01:52:55.049510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train batch 1/10\n",
      "Processed train batch 2/10\n",
      "Processed train batch 3/10\n",
      "Processed train batch 4/10\n",
      "Processed train batch 5/10\n",
      "Processed train batch 6/10\n",
      "Processed train batch 7/10\n",
      "Processed train batch 8/10\n",
      "Processed train batch 9/10\n",
      "Processed train batch 10/10\n",
      "Data saved to /kaggle/working/batch_output/activity_summary_train.csv\n",
      "Processed test batch 1/1\n",
      "Data saved to /kaggle/working/batch_output/activity_summary_test.csv\n"
     ]
    }
   ],
   "source": [
    "# paths for train and test actigraphy data\n",
    "train_path = '/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet'\n",
    "test_path = '/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet'\n",
    "output_dir = '/kaggle/working/batch_output'\n",
    "\n",
    "# initialize with batch size of 100\n",
    "processor = ActivityProcessor(train_path=train_path, test_path=test_path, output_dir=output_dir, batch_size=100)\n",
    "\n",
    "# process train data files\n",
    "processor.process_files(mode='train')\n",
    "\n",
    "# clear activity_summary_df\n",
    "processor.activity_summary_df = cudf.DataFrame()\n",
    "\n",
    "# process test data files\n",
    "processor.process_files(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a1f944",
   "metadata": {
    "papermill": {
     "duration": 0.003484,
     "end_time": "2024-10-27T01:55:27.150297",
     "exception": false,
     "start_time": "2024-10-27T01:55:27.146813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542e387",
   "metadata": {
    "papermill": {
     "duration": 0.003281,
     "end_time": "2024-10-27T01:55:27.156971",
     "exception": false,
     "start_time": "2024-10-27T01:55:27.153690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 161.313036,
   "end_time": "2024-10-27T01:55:28.080594",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-27T01:52:46.767558",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
